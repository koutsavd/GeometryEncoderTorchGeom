{
 "cells": [
  {
   "cell_type": "code",
   "id": "a8ec24aa62470869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T21:04:18.417778Z",
     "start_time": "2025-05-21T21:04:00.760574Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Linear, MSELoss, BCELoss, ReLU, Dropout, Sequential\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (needed for 3D plotting)\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T21:04:22.047926Z",
     "start_time": "2025-05-21T21:04:22.043153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# === Dataset Loader ===\n",
    "class CarGraphDataset(Dataset):\n",
    "    def __init__(self, root_dir, indices=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        all_files = sorted([f for f in self.root_dir.rglob(\"*.npz\") if \"_adj\" not in f.name])\n",
    "        self.files = [all_files[i] for i in indices] if indices else all_files\n",
    "        super().__init__()\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def get(self, idx):\n",
    "        feat_file = self.files[idx]\n",
    "        adj_file = feat_file.with_name(feat_file.name.replace(\".npz\", \"_adj.npz\"))\n",
    "        npz_data = np.load(feat_file)\n",
    "\n",
    "        x = torch.tensor(npz_data[\"x\"], dtype=torch.float32)\n",
    "        e = torch.tensor(npz_data[\"e\"], dtype=torch.float32)\n",
    "\n",
    "        a = sparse.load_npz(adj_file).tocoo()\n",
    "        edge_index = torch.tensor(np.vstack((a.row, a.col)), dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=e)\n"
   ],
   "id": "d609e41ac5aa72a5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T21:04:23.413099Z",
     "start_time": "2025-05-21T21:04:23.407663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# === Model ===\n",
    "class GraphAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Sequential(\n",
    "            GCNConv(in_channels, hidden_channels), ReLU(),\n",
    "            GCNConv(hidden_channels, hidden_channels), ReLU(),\n",
    "            GCNConv(hidden_channels, hidden_channels), ReLU()\n",
    "        )\n",
    "        self.encoder_lin = Sequential(\n",
    "            Linear(hidden_channels, latent_dim), ReLU(), Dropout(p=0.2))\n",
    "        self.decoder_lin = Sequential(\n",
    "            Linear(latent_dim, hidden_channels), ReLU(),\n",
    "            Linear(hidden_channels, in_channels)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index, batch=None):\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, GCNConv):\n",
    "                x = layer(x, edge_index)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        z = self.encoder_lin(x)\n",
    "        if batch is None:\n",
    "            # If batch is not provided, assume all nodes belong to one graph\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        z_graph = global_mean_pool(z, batch)\n",
    "        return z, z_graph\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder_lin(z)\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        z, z_graph = self.encode(x, edge_index, batch)\n",
    "        x_hat = self.decode(z)\n",
    "        return z, x_hat, z_graph\n"
   ],
   "id": "21a98b620ed5da3e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T21:04:26.319962Z",
     "start_time": "2025-05-21T21:04:26.315393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# === Training loop ===\n",
    "def run_epoch(model, loader, optimizer, device, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        z, x_hat, z_graph = model(batch.x, batch.edge_index, batch=batch.batch)\n",
    "\n",
    "        edge_index = batch.edge_index\n",
    "        z_i = z[edge_index[0]]\n",
    "        z_j = z[edge_index[1]]\n",
    "        dot_products = (z_i * z_j).sum(dim=1)\n",
    "        adj_pred = torch.sigmoid(dot_products)\n",
    "        adj_true = torch.ones_like(adj_pred)\n",
    "\n",
    "        loss_x = MSELoss()(x_hat, batch.x)\n",
    "        loss_a = BCELoss()(adj_pred, adj_true)\n",
    "        loss = loss_x + loss_a\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ],
   "id": "ff315818eb22c101",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T21:04:28.834735Z",
     "start_time": "2025-05-21T21:04:28.698974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# === Visualize original vs reconstructed geometry ===\n",
    "def plot_reconstruction(model, dataset, device, sample_idx=None):\n",
    "    model.eval()\n",
    "    idx = random.choice(range(len(dataset))) if sample_idx is None else sample_idx\n",
    "    data = dataset[idx].to(device)\n",
    "    with torch.no_grad():\n",
    "        z, x_hat, _ = model(data.x, data.edge_index,\n",
    "                            data.batch if hasattr(data, 'batch') else torch.zeros(data.x.size(0), dtype=torch.long,\n",
    "                                                                                  device=device))\n",
    "\n",
    "    x_orig = data.x.cpu().numpy()\n",
    "    x_recon = x_hat.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(min(3, x_orig.shape[1])):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.plot(x_orig[:, i], label='Original', alpha=0.7)\n",
    "        plt.plot(x_recon[:, i], label='Reconstructed', linestyle='--', alpha=0.7)\n",
    "        plt.title(f'Feature {i}')\n",
    "        plt.legend()\n",
    "    plt.suptitle(f'Reconstruction for sample #{idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reconstruction_sample.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# === Visualize 2D latent space ===\n",
    "def plot_latent_space(model, dataset, device):\n",
    "    model.eval()\n",
    "    zs = []\n",
    "    colors = []\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "        data = data.to(device)\n",
    "        batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device)  # if no batch attr\n",
    "        with torch.no_grad():\n",
    "            _, _, z_graph = model(data.x, data.edge_index, batch)\n",
    "        zs.append(z_graph.squeeze(0).cpu().numpy())\n",
    "        colors.append(i)\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    pca = PCA(n_components=2)\n",
    "    zs_2d = pca.fit_transform(zs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(zs_2d[:, 0], zs_2d[:, 1], c=colors, cmap='viridis', s=40, edgecolor='k')\n",
    "    plt.colorbar(scatter, label=\"Graph index\")\n",
    "    plt.title(\"2D PCA of Graph-Level Latent Embeddings\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"latent_projection.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_geometry_comparison(x_orig_np, x_recon_np, sample_idx=None):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax1.scatter(x_orig_np[:, 0], x_orig_np[:, 1], x_orig_np[:, 2],\n",
    "                c='blue', s=3, label='Original')\n",
    "    ax1.set_title(\"Original Geometry\")\n",
    "    ax1.set_xlabel(\"X\")\n",
    "    ax1.set_ylabel(\"Y\")\n",
    "    ax1.set_zlabel(\"Z\")\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax2.scatter(x_recon_np[:, 0], x_recon_np[:, 1], x_recon_np[:, 2],\n",
    "                c='red', s=3, label='Reconstructed')\n",
    "    ax2.set_title(\"Reconstructed Geometry\")\n",
    "    ax2.set_xlabel(\"X\")\n",
    "    ax2.set_ylabel(\"Y\")\n",
    "    ax2.set_zlabel(\"Z\")\n",
    "\n",
    "    fig.suptitle(f\"Sample #{sample_idx} - Original vs Reconstructed Geometry\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"geometry_comparison_{sample_idx}.png\")\n",
    "    plt.show()\n"
   ],
   "id": "d81ff638b59db58c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T21:24:26.828978Z",
     "start_time": "2025-05-21T21:05:33.040490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# === Main execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/Users/koutsavd/PycharmProjects/Geometry_GNN/Graphs\"\n",
    "    all_indices = list(range(len(list(Path(path).rglob(\"*[!_adj].npz\")))))\n",
    "    train_idx, valtest_idx = train_test_split(all_indices, test_size=0.3, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(valtest_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_set = CarGraphDataset(path, indices=train_idx)\n",
    "    val_set = CarGraphDataset(path, indices=val_idx)\n",
    "    test_set = CarGraphDataset(path, indices=test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=8)\n",
    "    test_loader = DataLoader(test_set, batch_size=8)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GraphAutoEncoder(in_channels=8, hidden_channels=64, latent_dim=256).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=\"runs/graph_ae\")\n",
    "    best_val_loss = float(\"inf\")\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        train_loss = run_epoch(model, train_loader, optimizer, device, train=True)\n",
    "        val_loss = run_epoch(model, val_loader, optimizer, device, train=False)\n",
    "\n",
    "        writer.add_scalars(\"Loss\", {\"train\": train_loss, \"val\": val_loss}, epoch)\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"checkpoints/best_model.pt\")"
   ],
   "id": "e216fdfe90b6ea06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 0.2366 | Val Loss: 0.1202\n",
      "Epoch 002 | Train Loss: 0.0964 | Val Loss: 0.0691\n",
      "Epoch 003 | Train Loss: 0.0655 | Val Loss: 0.0581\n",
      "Epoch 004 | Train Loss: 0.0593 | Val Loss: 0.0525\n",
      "Epoch 005 | Train Loss: 0.0534 | Val Loss: 0.0479\n",
      "Epoch 006 | Train Loss: 0.0508 | Val Loss: 0.0464\n",
      "Epoch 007 | Train Loss: 0.0489 | Val Loss: 0.0443\n",
      "Epoch 008 | Train Loss: 0.0471 | Val Loss: 0.0430\n",
      "Epoch 009 | Train Loss: 0.0461 | Val Loss: 0.0425\n",
      "Epoch 010 | Train Loss: 0.0456 | Val Loss: 0.0420\n",
      "Epoch 011 | Train Loss: 0.0452 | Val Loss: 0.0418\n",
      "Epoch 012 | Train Loss: 0.0450 | Val Loss: 0.0416\n",
      "Epoch 013 | Train Loss: 0.0447 | Val Loss: 0.0415\n",
      "Epoch 014 | Train Loss: 0.0446 | Val Loss: 0.0415\n",
      "Epoch 015 | Train Loss: 0.0444 | Val Loss: 0.0414\n",
      "Epoch 016 | Train Loss: 0.0444 | Val Loss: 0.0414\n",
      "Epoch 017 | Train Loss: 0.0442 | Val Loss: 0.0413\n",
      "Epoch 018 | Train Loss: 0.0442 | Val Loss: 0.0412\n",
      "Epoch 019 | Train Loss: 0.0441 | Val Loss: 0.0412\n",
      "Epoch 020 | Train Loss: 0.0441 | Val Loss: 0.0412\n",
      "Epoch 021 | Train Loss: 0.0439 | Val Loss: 0.0411\n",
      "Epoch 022 | Train Loss: 0.0439 | Val Loss: 0.0410\n",
      "Epoch 023 | Train Loss: 0.0438 | Val Loss: 0.0411\n",
      "Epoch 024 | Train Loss: 0.0438 | Val Loss: 0.0410\n",
      "Epoch 025 | Train Loss: 0.0437 | Val Loss: 0.0409\n",
      "Epoch 026 | Train Loss: 0.0437 | Val Loss: 0.0410\n",
      "Epoch 027 | Train Loss: 0.0436 | Val Loss: 0.0409\n",
      "Epoch 028 | Train Loss: 0.0435 | Val Loss: 0.0409\n",
      "Epoch 029 | Train Loss: 0.0435 | Val Loss: 0.0409\n",
      "Epoch 030 | Train Loss: 0.0435 | Val Loss: 0.0409\n",
      "Epoch 031 | Train Loss: 0.0435 | Val Loss: 0.0407\n",
      "Epoch 032 | Train Loss: 0.0434 | Val Loss: 0.0407\n",
      "Epoch 033 | Train Loss: 0.0434 | Val Loss: 0.0408\n",
      "Epoch 034 | Train Loss: 0.0433 | Val Loss: 0.0407\n",
      "Epoch 035 | Train Loss: 0.0433 | Val Loss: 0.0407\n",
      "Epoch 036 | Train Loss: 0.0432 | Val Loss: 0.0407\n",
      "Epoch 037 | Train Loss: 0.0432 | Val Loss: 0.0406\n",
      "Epoch 038 | Train Loss: 0.0432 | Val Loss: 0.0406\n",
      "Epoch 039 | Train Loss: 0.0431 | Val Loss: 0.0407\n",
      "Epoch 040 | Train Loss: 0.0431 | Val Loss: 0.0405\n",
      "Epoch 041 | Train Loss: 0.0431 | Val Loss: 0.0405\n",
      "Epoch 042 | Train Loss: 0.0430 | Val Loss: 0.0404\n",
      "Epoch 043 | Train Loss: 0.0430 | Val Loss: 0.0404\n",
      "Epoch 044 | Train Loss: 0.0429 | Val Loss: 0.0404\n",
      "Epoch 045 | Train Loss: 0.0429 | Val Loss: 0.0403\n",
      "Epoch 046 | Train Loss: 0.0428 | Val Loss: 0.0403\n",
      "Epoch 047 | Train Loss: 0.0428 | Val Loss: 0.0405\n",
      "Epoch 048 | Train Loss: 0.0428 | Val Loss: 0.0403\n",
      "Epoch 049 | Train Loss: 0.0427 | Val Loss: 0.0403\n",
      "Epoch 050 | Train Loss: 0.0427 | Val Loss: 0.0403\n",
      "Epoch 051 | Train Loss: 0.0428 | Val Loss: 0.0403\n",
      "Epoch 052 | Train Loss: 0.0427 | Val Loss: 0.0403\n",
      "Epoch 053 | Train Loss: 0.0427 | Val Loss: 0.0402\n",
      "Epoch 054 | Train Loss: 0.0427 | Val Loss: 0.0402\n",
      "Epoch 055 | Train Loss: 0.0427 | Val Loss: 0.0402\n",
      "Epoch 056 | Train Loss: 0.0426 | Val Loss: 0.0401\n",
      "Epoch 057 | Train Loss: 0.0425 | Val Loss: 0.0402\n",
      "Epoch 058 | Train Loss: 0.0425 | Val Loss: 0.0403\n",
      "Epoch 059 | Train Loss: 0.0426 | Val Loss: 0.0401\n",
      "Epoch 060 | Train Loss: 0.0425 | Val Loss: 0.0401\n",
      "Epoch 061 | Train Loss: 0.0424 | Val Loss: 0.0400\n",
      "Epoch 062 | Train Loss: 0.0424 | Val Loss: 0.0402\n",
      "Epoch 063 | Train Loss: 0.0424 | Val Loss: 0.0400\n",
      "Epoch 064 | Train Loss: 0.0424 | Val Loss: 0.0403\n",
      "Epoch 065 | Train Loss: 0.0424 | Val Loss: 0.0400\n",
      "Epoch 066 | Train Loss: 0.0423 | Val Loss: 0.0400\n",
      "Epoch 067 | Train Loss: 0.0423 | Val Loss: 0.0399\n",
      "Epoch 068 | Train Loss: 0.0423 | Val Loss: 0.0399\n",
      "Epoch 069 | Train Loss: 0.0422 | Val Loss: 0.0399\n",
      "Epoch 070 | Train Loss: 0.0422 | Val Loss: 0.0400\n",
      "Epoch 071 | Train Loss: 0.0422 | Val Loss: 0.0399\n",
      "Epoch 072 | Train Loss: 0.0421 | Val Loss: 0.0399\n",
      "Epoch 073 | Train Loss: 0.0421 | Val Loss: 0.0399\n",
      "Epoch 074 | Train Loss: 0.0422 | Val Loss: 0.0400\n",
      "Epoch 075 | Train Loss: 0.0421 | Val Loss: 0.0399\n",
      "Epoch 076 | Train Loss: 0.0421 | Val Loss: 0.0399\n",
      "Epoch 077 | Train Loss: 0.0421 | Val Loss: 0.0399\n",
      "Epoch 078 | Train Loss: 0.0422 | Val Loss: 0.0400\n",
      "Epoch 079 | Train Loss: 0.0421 | Val Loss: 0.0398\n",
      "Epoch 080 | Train Loss: 0.0421 | Val Loss: 0.0399\n",
      "Epoch 081 | Train Loss: 0.0420 | Val Loss: 0.0398\n",
      "Epoch 082 | Train Loss: 0.0420 | Val Loss: 0.0398\n",
      "Epoch 083 | Train Loss: 0.0420 | Val Loss: 0.0398\n",
      "Epoch 084 | Train Loss: 0.0419 | Val Loss: 0.0397\n",
      "Epoch 085 | Train Loss: 0.0419 | Val Loss: 0.0398\n",
      "Epoch 086 | Train Loss: 0.0419 | Val Loss: 0.0397\n",
      "Epoch 087 | Train Loss: 0.0419 | Val Loss: 0.0397\n",
      "Epoch 088 | Train Loss: 0.0419 | Val Loss: 0.0397\n",
      "Epoch 089 | Train Loss: 0.0418 | Val Loss: 0.0397\n",
      "Epoch 090 | Train Loss: 0.0418 | Val Loss: 0.0397\n",
      "Epoch 091 | Train Loss: 0.0418 | Val Loss: 0.0397\n",
      "Epoch 092 | Train Loss: 0.0419 | Val Loss: 0.0397\n",
      "Epoch 093 | Train Loss: 0.0418 | Val Loss: 0.0396\n",
      "Epoch 094 | Train Loss: 0.0418 | Val Loss: 0.0397\n",
      "Epoch 095 | Train Loss: 0.0417 | Val Loss: 0.0396\n",
      "Epoch 096 | Train Loss: 0.0417 | Val Loss: 0.0396\n",
      "Epoch 097 | Train Loss: 0.0417 | Val Loss: 0.0397\n",
      "Epoch 098 | Train Loss: 0.0417 | Val Loss: 0.0396\n",
      "Epoch 099 | Train Loss: 0.0417 | Val Loss: 0.0397\n",
      "Epoch 100 | Train Loss: 0.0417 | Val Loss: 0.0395\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    # === Test ===\n",
    "    model.load_state_dict(torch.load(\"checkpoints/best_model.pt\"))\n",
    "    test_loss = run_epoch(model, test_loader, optimizer, device, train=False)\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # === Visualizations ===\n",
    "    plot_latent_space(model, test_set, device)\n",
    "    plot_reconstruction(model, test_set, device)\n",
    "    # === Visualize full geometry ===\n",
    "    sample_idx = 5  # Or None for random\n",
    "    data = test_set[sample_idx].to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, x_hat, _ = model(data.x, data.edge_index)\n",
    "\n",
    "    x_orig_np = data.x[:, :3].cpu().numpy()\n",
    "    x_recon_np = x_hat[:, :3].cpu().numpy()\n",
    "\n",
    "    plot_geometry_comparison(x_orig_np, x_recon_np, sample_idx)\n"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
